---
layout: post
title: "Ensemble Learning"
date: 2021-11-25
description: "集成学习"
tag: 机器学习
katex: true
---

![2021-11-21-ML-trees-1](/assets/machine learning/2021-11-21-ML-trees-1.png)

集成学习是整合更多的模型，取长补短，最后通过组合策略得到结果。三个臭皮匠，顶个诸葛亮~

集成学习算法需要关注的问题

1. 个体学习器如何训练得到：改变训练数据的权值或概率分布
2. 如何将个体学习器组合

# 组合策略

## 平均法

对于数值类的回归预测问题，通常使用的结合策略是平均法，也就是说，对于若干个弱学习器的输出进行平均得到最终的预测输出。

最简单的平均是算术平均，最终预测为：
$$
H(x) = \frac{1}{T} \sum_1^T h_i(x)
$$
如果每个个体学习器有一个权重 `w`，最终预测为：
$$
H(X) = \sum_1^T w_ih_i(x) \\
\sum_1^T w_i = 1
$$
一般而言，个体学习器相差不大用简单平均法，相差较大用加权平均法。

## 投票法

对于分类问题，通常采用投票法。

相对多数投票法（Plurality Voting）：少数服从多数。

绝对多数投票法（Majority Voting）：少数服从多数的基础上，要求票过半数。

加权投票法（Weighted Voting）：每个学习器都有权重，最后将各个类别的加权票数求和，最大值对应的类别为最终类别。

## 学习法

对于学习法，代表方法是stacking，当使用stacking的结合策略时， 我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。

在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。

集成学习主要包括`bagging`和`boosting`两个派别：

# bagging

bagging 是 `bootstraping` 和 `aggregating` 的缩写，将多个弱分类器**并行**互不干扰，通过`voting`的方法得到预测结果。bagging 主要降`variance` 。

## 工作机制

1. 从原始样本中抽取k个训练集
2. k个训练集分别训练，得到k个模型
3. 将k个模型通过组合策略组合起来

bagging中代表算法为`Random Forest`。

## Random Forest

随机森林的“随机”主要体现在：

- `bootstraping`: 随机有放回的抽样
- 训练每棵树时，随机从M个特征中选取m个特征训练

```
优点：
1. 不用做特征选择，在训练完后，会给出比较重要的特征
2. 训练速度快，实现较简单，泛化能力强
缺点：
1.容易过拟合
```

# boosting

boosting为“提升”的意思，将弱分类器（决策树，逻辑回归）组合**串行**提升为一个强分类器。boosting主要降`bias`。

## 思想

绝大多数的提升方法都是通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类性能。

**提高**那些在前一轮被弱分类器**分错**的样本的权重，**减小**那些在前一轮被弱分类器**分对**的样本权重，使误分的样本在后续受到更多的关注。

## 工作机制

1. 串行：个体学习器 之前存在强依赖关系
2.  使用**加法模型**将弱分类器进行**线性组合**

boosting代表算法：`adaboostig`,`GBDT`,`Xgboost`,`lightGBM`





