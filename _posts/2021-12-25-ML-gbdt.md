---
layout: post
title: "GBDT"
date: 2021-12-25
description: "GBDT"
tag: 机器学习
katex: true  
---



提升树以分类树或回归树为基分类器的提升方法。被认为是性能最好的算法之一。

## 整体概况

### 加法模型

$$
f_M(x) = \sum_{m=1}^M T(x;\theta_m)
$$

 其中，$T(x,\theta_m)$为决策树，$\theta_m$为决策树的参数，$M$为树的个数。

### 损失函数

- 回归问题：平方误差损失
- 分类问题：对于二分类，指数损失函数；对于多分类，Softmax
- 一般决策问题：自定义损失函数

### 优化算法

提升树使用的是前向分步算法。

首先，确定初始提升树：

$$
f_0(x) = 0
$$

第m步的模型是
$$
f_m(x) = f_{m-1}(x)+T(x;\theta_m)
$$

其中$f_{m-1}$为当前模型，通过损失极小化确定下一棵决策树的参数$\theta_m$：

$$
\hat \theta_m = \arg \min_{\theta_m} \sum_{i=1}^{N} L(y_i,f_{m-1}(x)+T(x;\theta_m))
$$

注：上式的$\arg$ 表示使 损失函数最小时的 $\theta_m$值

## 对于二分类问题的提升树

相当于Adaboost算法的特殊情况：

1. 基分类器G(x) 为二类分类树
2. 基分类器权重$\alpha_m$全部为1

### 加法模型

$$
f_M(x) = \sum_{m=1}^M T(x;\theta_m)
$$

### 损失函数

指数损失函数

$$
L(y,f(x)) = \exp[-yf(x)]
$$

只要是指数损失函数，我们就可以使用其来调整样本数据的权值，从而让每个基分类器学到不同的内容

## 对于回归问题的提升树

### 基学习器

回归树

$$
T(x;\theta) = \sum_{j=1}^J c_j I(x \in R_j)
$$

其中，$\theta = \{(R_1,c_1),(R_2,c_2), \dots, (R_J,c_J)  \}$表示树的区域划分和各区域上的常数，J 是回归树的叶节点个数。

### 加法模型

$$
f_M(x) = \sum_{m=1}^M T(x;\theta_m)
$$

注意：没有权重。

### 损失函数

平方误差损失

$$
L(y,f(x)) = (y-f(x))^2
$$

将加法模型表达式代入

$$
L(y,f(x)) = (y-f(x))^2 =[y-f_{m-1}(x)-T(x;\theta_m)]^2 = [r -T(x;\theta_m) ]^2
$$

这里

$$
r = y - f_{m-1}(x)
$$

即为真实值减去预测值，为**残差**。

### 前向分步算法

**利用残差数据构建训练样本，让新的基学习器拟合训练样本**

$$
\hat \theta_m = \arg \min_{\theta_m} \sum_{i=1}^{N} L(y_i,f_{m-1}(x^{(i)}) + T(x^{(i)};\theta_m)) 
\\
=\arg \min_{\theta_m} \sum_{i=1}^{N}(r_m^{(i)} - T(x^{(i)};\theta_m))^2
$$

## 对于一般决策问题——GBDT

之前的两种提升树的损失函数都已经确定，如果对于一般决策问题，且对应一般损失函数而言，我们采取一种更为普遍的方法：梯度提升树（Gradient Boosting Decision Tress）。

### 基学习器

回归树

$$
T(x;\theta) = \sum_{j=1}^J c_j I(x \in R_j)
$$

### 加法模型

$$
f_M(x) = \sum_{m=1}^M T(x;\theta_m)
$$

### 损失函数

$$
L(y,f(x))
$$

### 前向分步算法 

核心目标是确保每增加一个基学习器，都要使总体损失越来越小，也就是说第m步要比第m-1步的损失要小，即

$$
L(y^{(i)},f_{m-1}(x^{(i)})) - L(y^{(i)},f_{m}(x^{(i)})) >0
$$

将损失函数进行处理（一阶泰勒展开），往核心目标上靠

![2021-12-25-ML-gbdt-1.png](/assets/machine learning/2021-12-25-ML-gbdt-1.png)

### 梯度提升

1. 计算当前损失函数的负梯度
   
   $$
   r_m(x,y) = -[\dfrac{\partial L(y,f_m(x))}{\partial f_m(x)}]_{f_m(x) =    f_{m-1}(x)}
   $$
   
2. 构造新的训练样本

   将$(x_i,y_i)$ 代入$r_m(x,y)$，将可得到$r_{mi}$，进而得到第m轮的训练数据集
   
   $$
   T_m = \{(x_1,r_{m1}),(x_2,r_{m2}),\dots,(x_N,r_{mN}) \}
   $$
   
3. 让当前的基学习器去拟合上述训练样本，得到$T(x;\theta_m)$

## GBDT解决二分类问题

GBDT是可以解决二分类问题的

### 面临问题

GBDT使用的是基分类器是回归树，得出连续值，其加法模型无法直接输出类别或者概率预估

### 解决方案

参考逻辑回归思想，使用`Sigmoid`函数将加法模型f(x)映射到0~1概率空间

$$
f_M(x) = \sum_{m=1}^M T(x;\theta_m)
\\ 
\hat y = \frac{1}{1+e^{-f_M(x)}}
\\
J = -\frac{1}{m} \sum_{i=1}^m y^{(i)} \log {\hat y}^{(i)} +  (1-y^{(i)}) \log (1-{\hat y}^{(i)})
$$

### 加法模型

$$
f_M(x) = \sum_{m=1}^M T(x;\theta_m)
$$

### 基学习器

回归树
$$
T(x;\theta) = \sum_{j=1}^J c_j I(x \in R_j)
$$

### 损失函数

$$
L(y,f_m(x)) = \log (1+e^{-f_m(x)})+(1-y)f_m(x)
$$

推导过程

![2021-12-25-ML-gbdt-2.png](/assets/machine learning/2021-12-25-ML-gbdt-2.png)

## Python实现

```python
from sklearn.ensemble import GradientBoostingClassifier

gbrt = GradientBoostingClassifier(random_state=1, learning_rate=0.1, n_estimators=5, max_depth=5, max_leaf_nodes)
```

**超参数：**

1. learning_rate：控制每棵树对前一棵树的错误的纠正强度；
2. n_estimators：树模型的数量，增大其数量会使得模型更加复杂；
3. max_depth：每棵树的最大深度；
4. max_leaf_nodes：每棵树最大的叶子结点个数；