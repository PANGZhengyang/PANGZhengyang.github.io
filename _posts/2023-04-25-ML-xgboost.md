---
layout: post
title: "Xgboost"
date: 2023-04-25
description: "Xgboost"
tag: 机器学习
katex: true  
---

Xgboost(eXtreme Gradient Boosting)在原有的GBDT基础上进行了改进，使得模型效果得到大大提升。

首先回顾一下GBDT的原理[详细内容见GBDT](https://pangzhengyang.github.io/2021/12/ML-gbdt/)。

将所有弱分类器的结果相加等于预测值，然后下一个弱分类器去拟合残差（真实值-预测值）数据样本。

# Xgboost目标函数

一般的目标函包含损失函数和正则化项。损失函数鼓励我们的模型尽量去拟合训练数据，使得模型有较小的bias，而正则化则鼓励更加简单的模型，减少过拟合，增加泛化性。
$$
Obj(\theta) = L(\theta) + \Omega(\theta)
$$
xgb的目标函数：
$$
Ojb = \sum_i l(\hat{y_i} - y_i) + \sum_k \Omega(f_k)
$$
其中，正则化项表示树的复杂程度，值越小，复杂程度越低，泛化能力越强，其表达式为：
$$
\Omega(f) = \gamma T + \frac{1}{2} \lambda \ || \omega \| ^2
$$
T - 叶子结点个数， w - 叶子结点的分数（结果）。直观上看，目标要求预测误差尽量小，且叶子节点T尽量少（γ控制叶子结点的个数），节点数值w尽量不极端（λ控制叶子节点的分数不会过大），防止过拟合。



## 损失函数

类似之前GBDT的套路，xgboost也是需要将多棵树的得分累加得到最终的预测得分（每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差）。
$$
\hat{y_i}^{(0)} = 0 
\\
\hat{y_i}^{(1)} = f_1(x_i) = \hat{y_i}^{(0)} + f_1(x_i)
\\
\hat{y_i}^{(2)} = f_1(x_i) +  f_2(x_i) = \hat{y_i}^{(1)} + f_2(x_i)
\\
\hat{y_i}^{(t)} = \sum^{t}_{k=1}f_k(x_i) = \hat{y_i}^{(t-1)} + f_t(x_i)
$$
如何选择每一轮加入什么函数 f 呢？答案是非常直接的，选取一个 f 来使得我们的目标函数尽量最大地降低。
$$
Obj^{(t)} = \sum_{i=1}^n l(y_i, \hat{y_i}^{(t)}  ) + \sum_{i=1}^t \Omega(f_i)
\\ = \sum_{i=1}^n l(y_i,\hat{y_i}^{(t-1)} + f_t(x_i) ) + \Omega(f_i) + constant
$$
考虑当 l 为平方误差的情况（相当于 $l(y,\hat{y}) = (y - \hat{y})^2$)，由于$y_i$为真实值，即已知，$\hat{y_i}^{(t-1)}$可由上一步t-1步中的$y_i^{t-2}$加上$f_{t-1}(x_i)$计算所求的，故也为已知值，所以上式子可以写成，
$$
Obj^{(t)} = \sum_{i=1}^n (y_i - (\hat{y_i}^{(t-1)} + f_t(x_i) ))^2 + \Omega(f_i) + constant
\\
= \sum_{i=1}^n [2(\hat{y_i}^{(t-1)} - y_i)f_t(x_i) + f_t(x_i)^2] + \Omega(f_i) + constant
$$
利用泰勒展开：



# XGBoost注意事项

## XGB与GBDT的不同点

1.GBDT是机器学习算法，XGBoost是该算法的工程实现。
2.在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模 型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。
3.GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。
4.传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器
5.传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。
6.传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺 失值的处理策略

## 是否需要做特征筛选

XGB可以做特征筛选，重要性特征排序。但是本质上XGB属于带有惩罚项的机器学习算法，所以不需要进行特征筛选，而逻辑回归需要。

## 是否需要对数据归一化

不需要，因为树模型是阶越的，不可导。



