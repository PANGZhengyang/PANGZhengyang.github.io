---
layout: post
title: "Decision Tree"
date: 2021-11-21
description: "树模型总结"
tag: 机器学习
katex: true
---

![2021-11-21-ML-trees-1](/assets/machine learning/2021-11-21-ML-trees-1.png)

# Decision Tree

## ID3

熵（Entropy）：一类当中的混乱程度，不确定性

信息熵 H(X) ：先验不确定性

$$
H(X) = -\sum_i P(x_i)\log P(x_i) \\
P(x_i) = \frac{|x_i|}{|s|} \ ,\ |s|为总例子数
$$

条件熵 $H(X|A)$ ：后验不确定性，接收了特征A后

$$
H(X|A) = -\sum_i P(A_i) \sum_i P(x_i|A_i) \log P(x_i|A_i)
$$

信息增益 Gain(X,A)

$$
Gain(X,A) = H(X) - H(X | A)
$$

ID3 算法使用的是**信息增益最大**作为划分依据。

```
优点：算法简单易懂
缺点：
1. 对噪声敏感
2. ID3 偏向于选择取值多的特征去划分
   Gain(X,A) = H(X)-H(X|A) H(X)固定，考虑极端情况，A特征下没个样本自成一类，则H(X|A)为0，信息增益最大，故ID3偏向于选择取值多的特征去划分
```

## C4.5

C4.5 使用的是**信息增益率**最大作为划分依据，加入了对属性划分的惩罚项，解决了ID3偏向于选择取值多的特征去划分的缺点。

T为数据集，其类别集合为${C_1,C_2,\dots ,C_k}$，选择一个属性V把T分为多个子集，V为互不重合的n个取值$v_1,v_2,\dots ,v_n$，T被划分为n个子集$T_1,T_2, \dots ,T_n$,其中$T_i$中所有实例的取值为$v_i$

$|T|$为T的例子数， $|T_i|$ 为 $v=v_i$的例子数，$|C_j|=freq(C_j, T)$ 为 $C_j$ 的类别数，$|C_j^v|$ 为 $v=v_i$ 的例子中 $C_j$ 类别例子数

举个例子来说，类别C：{NO, YES}，特征有{天气，温度，湿度}，选一个V(湿度)，$V_{湿度}$={v1(高)，v2(低)}，T被划分为两个子集{T1(湿度高), T2(湿度低)}，$|C_j^v|$表示在湿度的各个例子中，NO和YES的数量

类别$C_j$发生概率

$$
P(C_j) = \frac{|C_j|}{|T|}
$$

特征$v=v_i$发生的概率
$$
P(v_i) = \frac{|T_i|}{|T|}
$$

$v=v_i$的例子中$C_j$类别的条件概率
$$
P(C_j|v_i) = \frac{|C_j^v|}{|Ti|}
$$

类别的信息熵
$$
H(C) = -\sum_j P(C_j) \log P(C_j) = info(T)
$$

类别的条件熵：特征v把集合T分割后
$$
H(C|V) = -\sum_j P(v_j) \sum_j P(C_j|v_j) \log P(C_j|v_j)=info_v(T)
$$

信息增益
$$
Gain(C,V) = H(C) - H(C|V) = info(T) - info_v(T)
$$

特征v的信息熵
$$
H(V) = -\sum_j P(v_i) \log P(v_i) = info_{split}(v)
$$

信息增益率

$$
Gain\_ratio = \frac{Gain(C,V) }{H(V)}
$$

## CART

CART（Classfication and Regression Tree)，递归构建二叉树，主要有回归树和分类树。回归树用平方误差最小化准则，分类树用基尼系数最小化准则，进行特征划分，生成二叉树。

### 回归树

每个叶子结点都会输出一个预测值，且预测值一般是叶子所含训练样本在该节点上输出的均值。

回归树模型

$$
f(x) = \sum_{m=1}^M c_m I(x \in R_m)
$$

![2021-11-21-ML-trees-2.png](/assets/machine learning/2021-11-21-ML-trees-2.png)

树如何构建？

1. 树的深度如何决定

   第一种：确定叶子结点的个数或者深度

   第二种：确定叶子结点包含的最小样本数

   第三种：给定精度

2. 划分节点如何选取

   不同的节点划分条件，对应着不同的树，也就对应着不同的损失，从中选取损失最小的树即可。

3. 叶子节点代表的值$c_m$如何定

   **结论是：当每个叶子结点的$c_m$取值为该节点所有样本$y_i$的平均值时，得到的损失最小，即最优回归树。**

   首先，回归树的损失函数为评分误差
   
   $$
   J(y_i, f(x_i)) = \frac{1}{n} \sum_{i=1}^n(f(x_i)-y_i)^2
   $$
   
   用平方误差最小准则求解每个单元上的最有输出值，这就变成了一个优化问题。损失函数可以改写为
   
   $$
   J = \frac{1}{n} \sum_{i=1}^n(f(x_i)-y_i)^2 
   \\ = \frac{1}{n} \sum_{i=1}^n(\sum_{m=1}^M c_m I(x \in R_m) - y_i)^2 
   \\ = \frac{1}{n} \sum_{m=1}^M \sum_{x \in R_m}(c_m-y_i)^2
   $$
   
   优化目标
   
   $$
   c_m^* = \min_{c_m} \frac{1}{n} \sum_{m=1}^M \sum_{x_i \in R_m}(c_m-y_i)^2
   $$
   
   损失函数只包含一个参数$c_m$
   
   $$
   \dfrac{\partial J}{\partial c_m} = \dfrac{\partial  {\frac{1}{n} \sum_{m=1}^M \sum_{x_i \in R_m}(c_m-y_i)^2}}{\partial c_m}
   \\ = \dfrac{\partial \sum_{x_i \in R_m}(c_m -y_i)^2}{\partial c_m}
   \\ = 2 \sum_{x_i \in R_m}(c_m - y_i)
   $$
   
   设叶子结点$R_m$包含样本$N_m$个
   
   $$
   N_m c_m - \sum_{x_i \in R_m} y_i = 0
   \\ \longrightarrow c_m = \frac{\sum_{x_i \in R_m }y_i}{N_m}
   $$
   
   所以，当每个叶子结点的$c_m$取值为该节点所有样本$y_i$的平均值时，得到的损失最小，即最优回归树。

   

### 分类树

分类树用基尼系数选择最优特征，同时决定该特征的最优二值切分点。

假设某分类问题有K个类别，样本点属于第k类的概率为pk，则概率分布的基尼指数为

$$
Gini(p) = \sum_{k=1}^k p_k(1-p_k) = 1 - \sum_{k=1}^k p_k^2
$$

对于二分类问题，若样本点属于第1个类的概率为 p，则概率分布的基尼系数为

$$
Gini(p) = 2p(1-p)
$$

对于给定样本集合D，其基尼系数为
$$
Gini(D) = 1 - \sum_{k=1}^K (\frac{|C_k|}{D})^2 \\
其实就是 \ p_k = \frac{|C_k|}{D}
$$

$C_k$是D中属于第k类的样本子集，K是类的个数。

如果样本集 D 根据特征 A 是否取某一可能值 a 被分割为 $D_1$ 和 $D_2$ 两部分，即

$$
D_1 = \{ (x,y) \in D|A(x) = a \}, \ D2 = D-D_1
$$

在特征A的条件下，集合D的基尼系数为

$$
Gini(D,A) = \frac{|D_1|}{|D|}Gini(D_1) +\frac{|D_2|}{|D|}Gini(D_2)
$$

`Gini Index` 描述了混乱程度，不纯度。











